{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b791f198",
   "metadata": {},
   "source": [
    "### Pipiline\n",
    "\n",
    "1. Preprocesamiento de documentos\n",
    "Necesitas convertir PDFs de artículos científicos a texto\n",
    "Dividir el contenido en chunks (fragmentos) manejables\n",
    "Crear embeddings (representaciones vectoriales) de estos fragmentos\n",
    "2. Base de datos vectorial\n",
    "Almacena los embeddings para búsqueda eficiente\n",
    "Permite encontrar fragmentos similares a una consulta\n",
    "3. Pipeline de extracción de metadatos\n",
    "Define qué metadatos quieres extraer (título, autores, abstract, palabras clave, etc.)\n",
    "Usa el contexto recuperado para generar respuestas estructuradas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4c1a4",
   "metadata": {},
   "source": [
    "### Loading PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7547579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "file_path = \"/home/cristian/projects/rag_pae/data/pdfs/amazonica/A60.pdf\"\n",
    "doc = None\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    try:\n",
    "        if os.path.splitext(file_path)[1].lower() != '.pdf':\n",
    "            raise ValueError(\"The file is not a PDF.\")\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        doc = loader.load()\n",
    "        print(\"PDF loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7d11cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Caravanas, migrantes y desplazados: \\n'\n",
      " 'experiencias y debates en torno a las formas contemporáneas de movilidad '\n",
      " 'humana \\n'\n",
      " ' \\n'\n",
      " '175 \\n'\n",
      " 'Iberoforum. Revista de Ciencias Sociales de la Universidad Iberoamericana. \\n'\n",
      " 'Año XIV, No. 27, enero – junio 2019. \\n'\n",
      " 'Angela Yesenia Olaya Requene, pp. 175- 208, ISSN: 2007-0675 \\n'\n",
      " 'Universidad Iberoamericana Ciudad de México, www.ibero.mx/iberoforum/27 \\n'\n",
      " ' \\n'\n",
      " 'LA FRONTERA ENTRE COLOMBIA Y ECUADOR:  \\n'\n",
      " 'MOVILIDADES DE COMUNIDADES AFROCOLOMBIANAS  \\n'\n",
      " 'EN ESCENARIOS DEL NARCOTRÁFICO \\n'\n",
      " ' \\n'\n",
      " 'The Border Between Colombia and Ecuador: Mobilities of the Afro-Colombian '\n",
      " 'Communities \\n'\n",
      " ' in Drug Trafficking Contexts \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'Angela Yesenia Olaya Requene \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'Resumen \\n'\n",
      " 'l artículo analiza las dinámicas de movilidad \\n'\n",
      " 'local/trasnacional de comunidades \\n'\n",
      " 'afrocolombianas en la frontera entre Colombia y \\n'\n",
      " 'Ecuador por el Pacífico sur colombiano. Se describen los \\n'\n",
      " 'desplazamientos y las trayectorias de estas comunidades en \\n'\n",
      " 'el espacio local y trasnacional, así como el valor que ellas \\n'\n",
      " 'les confieren a las experiencias de reconfiguración de sus \\n'\n",
      " 'territorios y vida cotidiana marcada por la presencia de grupos armados y '\n",
      " 'cárteles del narcotráfico. A \\n'\n",
      " 'través de la etnografía multisituada se ensamblan los procesos históricos de '\n",
      " 'poblamiento de las \\n'\n",
      " 'comunidades afrocolombianas con los itinerarios y las trayectorias de las '\n",
      " 'violencias armadas que han \\n'\n",
      " 'hecho del espacio fronterizo: una ruta náutica del narcotráfico . Por '\n",
      " 'último, se reflexiona sobre la \\n'\n",
      " 'reconfiguración de la frontera como una “espacialidad de las violenc ias” '\n",
      " 'con su respectivo poder \\n'\n",
      " 'económico, militar y eficiencia de muerte para dominar y explotar a las '\n",
      " 'comunidades y territorios y, \\n'\n",
      " 'con ello, los intensos flujos de movilidad forzada de afrocolombianos hacia '\n",
      " 'Ecuador. \\n'\n",
      " ' \\n'\n",
      " 'Palabras claves: afrocolombianos, frontera, movilidades, narcotráfico. \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'Angela Yesenia Olaya Requene \\n'\n",
      " 'Doctora en Antropología Social, con maestría en \\n'\n",
      " 'Pedagogía y Política Educativa por la Universidad \\n'\n",
      " 'Nacional Autónoma de México ( UNAM). Licenciada en \\n'\n",
      " 'Sociología por la Universidad de Caldas (Colombia). Es \\n'\n",
      " 'investigadora asociada del Instituto de Investigaciones \\n'\n",
      " 'Afrolatinoamericanas de la Universidad de Harvard, y \\n'\n",
      " 'coordinadora académica del Certificado en Estudios \\n'\n",
      " 'Afrolatinoamericanos de la misma ins titución (2019 -\\n'\n",
      " '2020). Ha sido docente de las licenciaturas en \\n'\n",
      " 'Pedagogía, Relaciones Internacionales y Sociología de \\n'\n",
      " 'la UNAM. Sus líneas de investigación son: pueblos \\n'\n",
      " 'afrodescendientes, raza y racismo, territorios y \\n'\n",
      " 'migraciones forzadas.  \\n'\n",
      " 'Correo electrónico: yesenia-olaya@fas.harvard.edu. \\n'\n",
      " 'E')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(doc[1].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2868fa",
   "metadata": {},
   "source": [
    "### Chunking doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1822734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(doc)\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding= embeddings)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f40df138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de chunks semánticos: 23\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Inicializar embeddings para el semantic chunker\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Crear el semantic chunker\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Opciones: \"percentile\", \"standard_deviation\", \"interquartile\"\n",
    "    breakpoint_threshold_amount=95,  # Percentil 95 para encontrar breakpoints\n",
    "    number_of_chunks=None,  # Déjalo None para división automática\n",
    "    buffer_size=1  # Número de oraciones a considerar para el contexto\n",
    ")\n",
    "\n",
    "# Dividir el documento usando semantic chunking\n",
    "semantic_splits = semantic_splitter.split_documents(doc)\n",
    "\n",
    "print(f\"Número de chunks semánticos: {len(semantic_splits)}\")\n",
    "\n",
    "# Crear vectorstore con semantic chunks\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=semantic_splits, \n",
    "    embedding=embeddings,\n",
    "    collection_name=\"semantic_chunks\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db6352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Optional\n",
    "from datetime import date\n",
    "from enum import Enum\n",
    "\n",
    "class DocumentType(str, Enum):\n",
    "    ARTICLE = \"article\"\n",
    "    REVIEW = \"review\"\n",
    "    BOOK_CHAPTER = \"book_chapter\"\n",
    "    CONFERENCE_PAPER = \"conference_paper\"\n",
    "    THESIS = \"thesis\"\n",
    "    REPORT = \"report\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "class Language(str, Enum):\n",
    "    SPANISH = \"es\"\n",
    "    ENGLISH = \"en\"\n",
    "    PORTUGUESE = \"pt\"\n",
    "    FRENCH = \"fr\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "class Region(str, Enum):\n",
    "    AMAZONIA = \"amazonia\"\n",
    "    CARIBE = \"caribe\"\n",
    "    PACIFICO = \"pacifico\"\n",
    "    ANDINA = \"andina\"\n",
    "    ORINOQUIA = \"orinoquia\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "class ScienceDirectDocument(BaseModel):\n",
    "    # Metadatos básicos\n",
    "    authors: List[str] = Field(default_factory=list, description=\"Lista de autores del documento\", alias=\"autor\")\n",
    "    typology: str = Field(default=\"\", description=\"Tipología del documento\", alias=\"tipologia\")\n",
    "    year: int = Field(default=1900, description=\"Año de publicación\", ge=1900, le=2030, alias=\"año\")\n",
    "    title: str = Field(default=\"\", description=\"Título del documento\", alias=\"titulo\")\n",
    "    \n",
    "    # Contenido y análisis\n",
    "    objective: str = Field(default=\"\", description=\"Objetivo al que responde el documento\")\n",
    "    relevance: str = Field(default=\"\", description=\"Relevancia del documento para el estudio\")\n",
    "    document_type: DocumentType = Field(default=DocumentType.OTHER, description=\"Tipo de documento\")\n",
    "    gis_information: str = Field(default=\"\", description=\"Información SIG (Sistemas de Información Geográfica)\")\n",
    "    language: Language = Field(default=Language.SPANISH, description=\"Idioma del documento\")\n",
    "    keywords: List[str] = Field(default_factory=list, description=\"Palabras clave del documento\")\n",
    "    \n",
    "    # Referencias y ubicación\n",
    "    apa_reference: str = Field(default=\"\", description=\"Referencia en formato APA\")\n",
    "    pages: str = Field(default=\"\", description=\"Número de páginas or rango de páginas\")\n",
    "    abstract: str = Field(default=\"\", description=\"Resumen del documento\")\n",
    "    \n",
    "    # Análisis por región\n",
    "    regional_review_comments: str = Field(default=\"\", description=\"Comentario de revisión por región\")\n",
    "    relevant_pages_by_region: str = Field(default=\"\", description=\"Páginas relevantes por región\")\n",
    "    thesaurus_categories: List[str] = Field(default_factory=list, description=\"Categorías del tesauro\")\n",
    "    \n",
    "    # Análisis de victimización y contexto\n",
    "    victimizing_event: str = Field(default=\"\", description=\"Hecho victimizante identificado\")\n",
    "    damage: str = Field(default=\"\", description=\"Daño o impacto identificado\")\n",
    "    actor: str = Field(default=\"\", description=\"Actor involucrado\")\n",
    "    location: str = Field(default=\"\", description=\"Lugar general\")\n",
    "    specific_location_with_review: str = Field(default=\"\", description=\"Lugar específico con revisión\")\n",
    "    context_year: int = Field(default=1900, description=\"Año del contexto analizado\", ge=1900, le=2030)\n",
    "    region: Region = Field(default=Region.OTHER, description=\"Región geográfica\")\n",
    "    \n",
    "    # Metadatos técnicos adicionales (EXTRA - no requeridos pero útiles)\n",
    "    doi: str = Field(default=\"\", description=\"DOI del documento\")\n",
    "    journal: str = Field(default=\"\", description=\"Revista o fuente de publicación\")\n",
    "\n",
    "    model_config = {\n",
    "        \"use_enum_values\": True,\n",
    "        \"populate_by_name\": True  # V2 equivalent of allow_population_by_field_name\n",
    "    }\n",
    "        \n",
    "    @field_validator('year', 'context_year')\n",
    "    @classmethod\n",
    "    def validate_years(cls, v):\n",
    "        \"\"\"Validar que los años estén en rango razonable\"\"\"\n",
    "        if v < 1900 or v > 2030:\n",
    "            return 1900  # Valor por defecto si está fuera de rango\n",
    "        return v\n",
    "    \n",
    "    @field_validator('authors', 'keywords', 'thesaurus_categories')\n",
    "    @classmethod\n",
    "    def validate_lists(cls, v):\n",
    "        \"\"\"Limpiar listas de strings vacíos\"\"\"\n",
    "        if isinstance(v, list):\n",
    "            return [item.strip() for item in v if item and item.strip()]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecca0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "llm = OllamaLLM(model=\"qwen2.5vl:7b\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "197b2254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "('```json\\n'\n",
      " '{\\n'\n",
      " '  \"autor\": [\"Simmel\", \"Gilbert\", \"Prieto\", \"Rocha\", \"Marín C.\", \"Schutz\", '\n",
      " '\"Olga Sabido Ramos\"],\\n'\n",
      " '  \"tipologia\": \"\",\\n'\n",
      " '  \"año\": 2012,\\n'\n",
      " '  \"titulo\": \"El extranjero en el contexto de la migración\",\\n'\n",
      " '  \"objective\": \"Determinar el grado de conflicto social generado en espacios '\n",
      " 'como el Corregimiento de Jaqué.\",\\n'\n",
      " '  \"relevance\": \"Complementar historias de vida que permitan plasmar cómo ha '\n",
      " 'sido su proceso de integración y cómo se ha desarrollado el conflicto social '\n",
      " 'en esta parte del territorio panameño.\",\\n'\n",
      " '  \"document_type\": \"article\",\\n'\n",
      " '  \"gis_information\": \"\",\\n'\n",
      " '  \"language\": \"es\",\\n'\n",
      " '  \"keywords\": [\"extranjero\", \"migración\", \"conflicto social\", \"integración\", '\n",
      " '\"territorio panameño\"],\\n'\n",
      " '  \"apa_reference\": \"Simmel, G. (2012). El extranjero en el contexto de la '\n",
      " 'migración. Introducción. Cuadernos de Investigación Social, 23(1), 1-10.\",\\n'\n",
      " '  \"pages\": \"54\",\\n'\n",
      " '  \"abstract\": \"Hasta aquí se han esbozado ciertos elementos para determinar '\n",
      " 'el grado de conflicto social generado en espacios como el Corregimiento de '\n",
      " 'Jaqué. El tema, sin embargo, debe ser complementado con historias de vida '\n",
      " 'que permitan plasmar cómo ha sido su proceso de integración y cómo – y con '\n",
      " 'qué resultados - se ha desarrollado el conflicto social en esta parte del '\n",
      " 'territorio panameño.\",\\n'\n",
      " '  \"regional_review_comments\": \"\",\\n'\n",
      " '  \"relevant_pages_by_region\": \"\",\\n'\n",
      " '  \"thesaurus_categories\": [\"conflicto social\", \"integración\", \"territorio '\n",
      " 'panameño\"],\\n'\n",
      " '  \"victimizing_event\": \"conflicto armado colombiano\",\\n'\n",
      " '  \"damage\": \"abandono del lugar de origen\",\\n'\n",
      " '  \"actor\": \"\",\\n'\n",
      " '  \"location\": \"Corregimiento de Jaqué\",\\n'\n",
      " '  \"specific_location_with_review\": \"\",\\n'\n",
      " '  \"context_year\": 2012,\\n'\n",
      " '  \"region\": \"pacifico\",\\n'\n",
      " '  \"doi\": \"\",\\n'\n",
      " '  \"journal\": \"Introducción\"\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "schema = ScienceDirectDocument.model_json_schema()\n",
    "\n",
    "system_prompt = \"\"\"You are a specialized academic document metadata extractor with expertise in scientific literature analysis.\n",
    "\n",
    "EXTRACTION GUIDELINES:\n",
    "- Process the document systematically from header to conclusion\n",
    "- Cross-reference information across different sections\n",
    "- Prioritize explicit over inferred information\n",
    "- Maintain field-specific formatting requirements\n",
    "\n",
    "FIELD EXTRACTION PRIORITY:\n",
    "1. TITLE & AUTHORS: Extract from header, first page, or abstract section\n",
    "2. YEAR: Look for publication date, copyright, or journal volume\n",
    "3. ABSTRACT: Usually found after title/authors or in dedicated section\n",
    "4. KEYWORDS: May appear after abstract or as separate section\n",
    "5. DOCUMENT TYPE: Infer from publication format and content structure\n",
    "\n",
    "FIELD-SPECIFIC INSTRUCTIONS:\n",
    "- authors: Extract full names, handle \"et al.\" appropriately\n",
    "- title: Include subtitles, remove formatting artifacts\n",
    "- abstract: Full text, not just first sentence\n",
    "- keywords: Separate by commas, normalize formatting\n",
    "- language: Detect from content, not just metadata\n",
    "- region: Infer from geographic references and study locations\n",
    "- year: Validate range 1900-2030, use context_year for historical studies\n",
    "\n",
    "QUALITY CHECKS:\n",
    "- Verify author names match document header\n",
    "- Ensure year consistency across document\n",
    "- Cross-check abstract with document content\n",
    "- Validate geographic references for region classification\n",
    "\n",
    "CONTEXT SECTIONS:\n",
    "{context}\n",
    "\n",
    "REQUIRED JSON SCHEMA:\n",
    "{schema}\n",
    "\n",
    "OUTPUT (JSON only, no explanations):\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template (\n",
    "    system_prompt)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"schema\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = (rag_chain.invoke(str(schema)))\n",
    "print(\"Output:\")\n",
    "pprint.pprint(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1ee032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDACIÓN DE METADATOS ===\n",
      "✅ Todos los campos críticos están presentes\n",
      "📊 Completitud: 23/25 campos (92.0%)\n",
      "\n",
      "📝 Campos que podrían necesitar revisión manual:\n",
      "   - regional_review_comments\n",
      "   - relevant_pages_by_region\n",
      "\n",
      "=== DOCUMENTO FINAL VALIDADO ===\n",
      "Título: Migración transfronteriza indígena en Darién, Panamá\n",
      "Autores: Bilbao, I., R. Falla, E. Valdés, M. M. Callaghan, N. Chaqui, F. Checa O., CODHES, Gálvez, A., N. García Canclini, J. García Casares, R. González Guzmán, L. Guarín, P. H. Herlihy, A. Pastor N., B. Quintero, W. Hughes, J. Rodríguez J., G. Rudolf\n",
      "Año: 2004\n",
      "Tipo: article\n",
      "Región: pacifico\n",
      "Idioma: es\n"
     ]
    }
   ],
   "source": [
    "# Validación y refinamiento de metadatos\n",
    "def validate_and_refine_metadata(metadata_dict):\n",
    "    \"\"\"Valida y refina los metadatos extraídos\"\"\"\n",
    "    \n",
    "    print(\"=== VALIDACIÓN DE METADATOS ===\")\n",
    "    \n",
    "    # Verificar campos críticos\n",
    "    critical_fields = ['title', 'authors', 'year']\n",
    "    missing_critical = [field for field in critical_fields \n",
    "                       if not metadata_dict.get(field) or \n",
    "                       (isinstance(metadata_dict.get(field), list) and not metadata_dict[field])]\n",
    "    \n",
    "    if missing_critical:\n",
    "        print(f\"⚠️  Campos críticos faltantes: {missing_critical}\")\n",
    "    else:\n",
    "        print(\"✅ Todos los campos críticos están presentes\")\n",
    "    \n",
    "    # Mostrar estadísticas de completitud\n",
    "    total_fields = len(ScienceDirectDocument.model_fields)\n",
    "    filled_fields = sum(1 for key, value in metadata_dict.items() \n",
    "                       if value and value != [] and value != \"\" and value != 1900)\n",
    "    \n",
    "    completeness = (filled_fields / total_fields) * 100\n",
    "    print(f\"📊 Completitud: {filled_fields}/{total_fields} campos ({completeness:.1f}%)\")\n",
    "    \n",
    "    # Mostrar campos vacíos para revisión manual\n",
    "    empty_fields = [key for key, value in metadata_dict.items() \n",
    "                   if not value or value == [] or value == \"\" or value == 1900]\n",
    "    \n",
    "    if empty_fields:\n",
    "        print(f\"\\n📝 Campos que podrían necesitar revisión manual:\")\n",
    "        for field in empty_fields[:10]:  # Mostrar solo los primeros 10\n",
    "            print(f\"   - {field}\")\n",
    "        if len(empty_fields) > 10:\n",
    "            print(f\"   ... y {len(empty_fields) - 10} más\")\n",
    "    \n",
    "    return metadata_dict\n",
    "\n",
    "# Validar los metadatos extraídos\n",
    "validated_metadata = validate_and_refine_metadata(metadata_result)\n",
    "\n",
    "# Crear instancia final del documento\n",
    "final_document = ScienceDirectDocument(**validated_metadata)\n",
    "print(f\"\\n=== DOCUMENTO FINAL VALIDADO ===\")\n",
    "print(f\"Título: {final_document.title}\")\n",
    "print(f\"Autores: {', '.join(final_document.authors) if final_document.authors else 'No especificado'}\")\n",
    "print(f\"Año: {final_document.year}\")\n",
    "print(f\"Tipo: {final_document.document_type}\")\n",
    "print(f\"Región: {final_document.region}\")\n",
    "print(f\"Idioma: {final_document.language}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd83fa9e",
   "metadata": {},
   "source": [
    "## 🚀 Mejoras Implementadas para el RAG\n",
    "\n",
    "### 1. Retriever Híbrido con Estrategias Múltiples\n",
    "- Búsqueda especializada por tipo de información\n",
    "- Combinación de chunks estratégicos\n",
    "- Priorización de primeras páginas para metadatos básicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4559619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever híbrido inicializado\n"
     ]
    }
   ],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"Retriever híbrido con estrategias múltiples para mejor extracción de metadatos\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, documents):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.documents = documents\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "    \n",
    "    def get_strategic_chunks(self, query_type=\"general\"):\n",
    "        \"\"\"Retrieve chunks based on extraction strategy\"\"\"\n",
    "        \n",
    "        # Estrategias específicas por tipo de información\n",
    "        strategies = {\n",
    "            \"basic_metadata\": [\n",
    "                \"title author publication year journal\",\n",
    "                \"abstract summary introduction\",\n",
    "                \"first page header metadata\"\n",
    "            ],\n",
    "            \"content_analysis\": [\n",
    "                \"objective methodology research question\",\n",
    "                \"results conclusions findings\",\n",
    "                \"keywords terms concepts\"\n",
    "            ],\n",
    "            \"geographic_context\": [\n",
    "                \"location region geographic area\",\n",
    "                \"study site fieldwork territory\",\n",
    "                \"amazonia caribe pacifico andina orinoquia\"\n",
    "            ],\n",
    "            \"victimization\": [\n",
    "                \"violence conflict victim damage\",\n",
    "                \"actor perpetrator impact effect\",\n",
    "                \"social political economic\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        # Obtener chunks para cada estrategia\n",
    "        for strategy_queries in strategies.values():\n",
    "            for query in strategy_queries:\n",
    "                try:\n",
    "                    chunks = self.retriever.get_relevant_documents(query)\n",
    "                    all_chunks.extend(chunks[:2])  # Top 2 por query\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en query '{query}': {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Agregar siempre las primeras páginas (metadatos básicos)\n",
    "        first_pages = [doc for doc in self.documents[:3]]\n",
    "        all_chunks.extend(first_pages)\n",
    "        \n",
    "        # Deduplicar y ordenar por relevancia\n",
    "        unique_chunks = []\n",
    "        seen_content = set()\n",
    "        \n",
    "        for chunk in all_chunks:\n",
    "            content_hash = hash(chunk.page_content[:100])\n",
    "            if content_hash not in seen_content:\n",
    "                unique_chunks.append(chunk)\n",
    "                seen_content.add(content_hash)\n",
    "        \n",
    "        return unique_chunks[:15]  # Limitar a 15 chunks más relevantes\n",
    "\n",
    "# Implementar el retriever híbrido\n",
    "hybrid_retriever = HybridRetriever(vectorstore, doc)\n",
    "print(\"✅ Retriever híbrido inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4684c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPassExtractor:\n",
    "    \"\"\"Extractor de metadatos con múltiples pasadas especializadas\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, retriever):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        \n",
    "    def extract_metadata_multipass(self):\n",
    "        \"\"\"Extracción en múltiples pasadas especializadas\"\"\"\n",
    "        \n",
    "        print(\"🔄 Iniciando extracción multi-pasada...\")\n",
    "        \n",
    "        # Pasada 1: Metadatos básicos\n",
    "        print(\"📝 Pasada 1: Extrayendo metadatos básicos...\")\n",
    "        basic_metadata = self._extract_basic_metadata()\n",
    "        \n",
    "        # Pasada 2: Análisis de contenido\n",
    "        print(\"📚 Pasada 2: Analizando contenido...\")\n",
    "        content_analysis = self._extract_content_analysis()\n",
    "        \n",
    "        # Pasada 3: Contexto geográfico y regional\n",
    "        print(\"🌍 Pasada 3: Determinando contexto geográfico...\")\n",
    "        geographic_context = self._extract_geographic_context()\n",
    "        \n",
    "        # Pasada 4: Análisis de victimización (si aplica)\n",
    "        print(\"⚖️ Pasada 4: Analizando contexto de victimización...\")\n",
    "        victimization_context = self._extract_victimization_context()\n",
    "        \n",
    "        # Combinar y validar\n",
    "        combined_metadata = {\n",
    "            **basic_metadata,\n",
    "            **content_analysis, \n",
    "            **geographic_context,\n",
    "            **victimization_context\n",
    "        }\n",
    "        \n",
    "        return self._validate_and_cross_check(combined_metadata)\n",
    "    \n",
    "    def _extract_basic_metadata(self):\n",
    "        \"\"\"Extrae título, autores, año, tipo de documento\"\"\"\n",
    "        \n",
    "        prompt = \"\"\"Extract ONLY basic bibliographic metadata from this academic document.\n",
    "\n",
    "Focus exclusively on: title, authors, publication year, document_type, language, journal.\n",
    "\n",
    "SPECIFIC INSTRUCTIONS:\n",
    "- title: Complete title without artifacts or formatting issues\n",
    "- authors: Full author names, handle \"et al.\" appropriately\n",
    "- year: Publication year between 1900-2030\n",
    "- document_type: Choose from: article, review, book_chapter, conference_paper, thesis, report, other\n",
    "- language: Detect from content (es, en, pt, fr, other)\n",
    "- journal: Source publication or journal name\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "Return ONLY valid JSON with these fields. If information is not available, use appropriate defaults:\n",
    "{{\n",
    "    \"title\": \"\",\n",
    "    \"authors\": [],\n",
    "    \"year\": 1900,\n",
    "    \"document_type\": \"other\",\n",
    "    \"language\": \"es\",\n",
    "    \"journal\": \"\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        chunks = self.retriever.get_strategic_chunks(\"basic_metadata\")\n",
    "        context = format_docs(chunks)\n",
    "        \n",
    "        result = self._run_extraction(prompt, context)\n",
    "        return self._parse_json_safely(result)\n",
    "    \n",
    "    def _extract_content_analysis(self):\n",
    "        \"\"\"Extrae abstract, keywords, objetivos\"\"\"\n",
    "        \n",
    "        prompt = \"\"\"Extract content analysis metadata from this document.\n",
    "\n",
    "Focus on: abstract, keywords, objective, relevance, typology.\n",
    "\n",
    "SPECIFIC INSTRUCTIONS:\n",
    "- abstract: Full abstract or summary text\n",
    "- keywords: List of key terms and concepts\n",
    "- objective: Research objective or purpose\n",
    "- relevance: Relevance to the study\n",
    "- typology: Document typology/classification\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "    \"abstract\": \"\",\n",
    "    \"keywords\": [],\n",
    "    \"objective\": \"\",\n",
    "    \"relevance\": \"\",\n",
    "    \"typology\": \"\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        chunks = self.retriever.get_strategic_chunks(\"content_analysis\")\n",
    "        context = format_docs(chunks)\n",
    "        \n",
    "        result = self._run_extraction(prompt, context)\n",
    "        return self._parse_json_safely(result)\n",
    "    \n",
    "    def _extract_geographic_context(self):\n",
    "        \"\"\"Extrae contexto geográfico y regional\"\"\"\n",
    "        \n",
    "        prompt = \"\"\"Extract geographic and regional context from this document.\n",
    "\n",
    "Focus on: region, location, specific_location_with_review, gis_information.\n",
    "\n",
    "SPECIFIC INSTRUCTIONS:\n",
    "- region: Choose from: amazonia, caribe, pacifico, andina, orinoquia, other\n",
    "- location: General location or area\n",
    "- specific_location_with_review: Specific places mentioned with details\n",
    "- gis_information: Any GIS or geographic information systems data\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "    \"region\": \"other\",\n",
    "    \"location\": \"\",\n",
    "    \"specific_location_with_review\": \"\",\n",
    "    \"gis_information\": \"\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        chunks = self.retriever.get_strategic_chunks(\"geographic_context\")\n",
    "        context = format_docs(chunks)\n",
    "        \n",
    "        result = self._run_extraction(prompt, context)\n",
    "        return self._parse_json_safely(result)\n",
    "    \n",
    "    def _extract_victimization_context(self):\n",
    "        \"\"\"Extrae contexto de victimización y actores\"\"\"\n",
    "        \n",
    "        prompt = \"\"\"Extract victimization and social context from this document.\n",
    "\n",
    "Focus on: victimizing_event, damage, actor, context_year.\n",
    "\n",
    "SPECIFIC INSTRUCTIONS:\n",
    "- victimizing_event: Any victimizing events or violence described\n",
    "- damage: Impacts, damages, or consequences identified  \n",
    "- actor: Actors, perpetrators, or entities involved\n",
    "- context_year: Year of the events/context analyzed (1900-2030)\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "    \"victimizing_event\": \"\",\n",
    "    \"damage\": \"\",\n",
    "    \"actor\": \"\",\n",
    "    \"context_year\": 1900\n",
    "}}\"\"\"\n",
    "        \n",
    "        chunks = self.retriever.get_strategic_chunks(\"victimization\")\n",
    "        context = format_docs(chunks)\n",
    "        \n",
    "        result = self._run_extraction(prompt, context)\n",
    "        return self._parse_json_safely(result)\n",
    "    \n",
    "    def _run_extraction(self, prompt_template, context):\n",
    "        \"\"\"Ejecuta una extracción con el prompt dado\"\"\"\n",
    "        try:\n",
    "            prompt = PromptTemplate.from_template(prompt_template)\n",
    "            chain = prompt | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\"context\": context})\n",
    "            return result.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error en extracción: {e}\")\n",
    "            return \"{}\"\n",
    "    \n",
    "    def _parse_json_safely(self, json_str):\n",
    "        \"\"\"Parse JSON de forma segura\"\"\"\n",
    "        try:\n",
    "            # Limpiar el string si tiene texto extra\n",
    "            json_str = json_str.strip()\n",
    "            if json_str.startswith('```json'):\n",
    "                json_str = json_str[7:]\n",
    "            if json_str.endswith('```'):\n",
    "                json_str = json_str[:-3]\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parseando JSON: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _validate_and_cross_check(self, metadata):\n",
    "        \"\"\"Validación cruzada de metadatos extraídos\"\"\"\n",
    "        \n",
    "        # Verificar consistencia de año\n",
    "        if metadata.get('year') and metadata.get('context_year'):\n",
    "            if abs(metadata['year'] - metadata['context_year']) > 50:\n",
    "                metadata['context_year'] = metadata['year']\n",
    "        \n",
    "        # Validar rangos de años\n",
    "        for year_field in ['year', 'context_year']:\n",
    "            if metadata.get(year_field):\n",
    "                if metadata[year_field] < 1900 or metadata[year_field] > 2030:\n",
    "                    metadata[year_field] = 1900\n",
    "        \n",
    "        # Limpiar listas vacías\n",
    "        list_fields = ['authors', 'keywords', 'thesaurus_categories']\n",
    "        for field in list_fields:\n",
    "            if field in metadata and isinstance(metadata[field], list):\n",
    "                metadata[field] = [item.strip() for item in metadata[field] if item and item.strip()]\n",
    "        \n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataValidator:\n",
    "    \"\"\"Validador y mejorador inteligente de metadatos\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, retriever):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        \n",
    "    def validate_and_enhance(self, metadata):\n",
    "        \"\"\"Validación y mejora de metadatos\"\"\"\n",
    "        \n",
    "        print(\"🔍 Iniciando validación y mejora de metadatos...\")\n",
    "        \n",
    "        # 1. Validar campos críticos faltantes\n",
    "        enhanced_metadata = self._fill_missing_critical_fields(metadata)\n",
    "        \n",
    "        # 2. Normalizar y limpiar datos\n",
    "        enhanced_metadata = self._normalize_fields(enhanced_metadata)\n",
    "        \n",
    "        # 3. Validación semántica\n",
    "        enhanced_metadata = self._semantic_validation(enhanced_metadata)\n",
    "        \n",
    "        return enhanced_metadata\n",
    "    \n",
    "    def _fill_missing_critical_fields(self, metadata):\n",
    "        \"\"\"Intenta llenar campos críticos faltantes con queries específicas\"\"\"\n",
    "        \n",
    "        critical_fields = {\n",
    "            'title': \"document title, paper title, article title, título del documento\",\n",
    "            'authors': \"authors, writers, researchers, by, written by, autores, autor\",\n",
    "            'year': \"publication year, published, date, copyright, año de publicación\",\n",
    "            'abstract': \"abstract, summary, resumen, síntesis, introducción\"\n",
    "        }\n",
    "        \n",
    "        for field, query in critical_fields.items():\n",
    "            if not metadata.get(field) or metadata[field] in [\"\", [], 1900]:\n",
    "                print(f\"🔍 Buscando {field} faltante...\")\n",
    "                \n",
    "                result = self._targeted_extraction(field, query)\n",
    "                if result and result != \"No disponible\":\n",
    "                    if field == 'authors' and isinstance(result, str):\n",
    "                        # Convertir string de autores a lista\n",
    "                        metadata[field] = [author.strip() for author in result.split(',') if author.strip()]\n",
    "                    elif field == 'year' and isinstance(result, str):\n",
    "                        # Extraer año del string\n",
    "                        import re\n",
    "                        year_match = re.search(r'\\b(19|20)\\d{2}\\b', result)\n",
    "                        if year_match:\n",
    "                            metadata[field] = int(year_match.group())\n",
    "                    else:\n",
    "                        metadata[field] = result\n",
    "                    print(f\"✅ {field} encontrado: {str(result)[:50]}...\")\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _targeted_extraction(self, field, query):\n",
    "        \"\"\"Extracción dirigida para un campo específico\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Extract ONLY the {field} from this document context.\n",
    "\n",
    "Search for: {query}\n",
    "\n",
    "Be precise and extract only the requested information.\n",
    "If not found, respond with \"No disponible\".\n",
    "\n",
    "CONTEXT: {{context}}\n",
    "\n",
    "{field.upper()}:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            docs = self.retriever.retriever.get_relevant_documents(query)\n",
    "            context = format_docs(docs[:5])\n",
    "            \n",
    "            chain = PromptTemplate.from_template(prompt) | self.llm | StrOutputParser()\n",
    "            result = chain.invoke({\"context\": context})\n",
    "            \n",
    "            return result.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error en extracción dirigida: {e}\")\n",
    "            return \"No disponible\"\n",
    "    \n",
    "    def _normalize_fields(self, metadata):\n",
    "        \"\"\"Normaliza formato de campos\"\"\"\n",
    "        \n",
    "        # Normalizar autores\n",
    "        if isinstance(metadata.get('authors'), str):\n",
    "            authors = [author.strip() for author in metadata['authors'].split(',')]\n",
    "            metadata['authors'] = [author for author in authors if author]\n",
    "        \n",
    "        # Normalizar keywords\n",
    "        if isinstance(metadata.get('keywords'), str):\n",
    "            keywords = [kw.strip() for kw in metadata['keywords'].split(',')]\n",
    "            metadata['keywords'] = [kw for kw in keywords if kw]\n",
    "        \n",
    "        # Limpiar campos de texto\n",
    "        text_fields = ['title', 'abstract', 'objective', 'relevance']\n",
    "        for field in text_fields:\n",
    "            if metadata.get(field):\n",
    "                metadata[field] = metadata[field].strip().replace('\\n', ' ')\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _semantic_validation(self, metadata):\n",
    "        \"\"\"Validación semántica de coherencia\"\"\"\n",
    "        \n",
    "        print(\"🧠 Realizando validación semántica...\")\n",
    "        \n",
    "        # Validar coherencia título-abstract\n",
    "        if metadata.get('title') and metadata.get('abstract'):\n",
    "            if len(metadata['abstract']) > 50 and metadata['title'].lower() not in metadata['abstract'].lower():\n",
    "                print(\"⚠️ Posible inconsistencia: título no aparece en abstract\")\n",
    "        \n",
    "        # Validar coherencia año-contexto\n",
    "        if metadata.get('year') and metadata.get('context_year'):\n",
    "            if abs(metadata['year'] - metadata['context_year']) > 50:\n",
    "                print(f\"⚠️ Gran diferencia entre año de publicación ({metadata['year']}) y año de contexto ({metadata['context_year']})\")\n",
    "        \n",
    "        # Validar región vs ubicación\n",
    "        if metadata.get('region') and metadata.get('location'):\n",
    "            region_keywords = {\n",
    "                'amazonia': ['amazonas', 'amazónico', 'selva'],\n",
    "                'caribe': ['caribe', 'costa', 'atlántico'],\n",
    "                'pacifico': ['pacífico', 'costa pacífica'],\n",
    "                'andina': ['andes', 'andino', 'montaña'],\n",
    "                'orinoquia': ['orinoco', 'llanos']\n",
    "            }\n",
    "            \n",
    "            region = metadata['region'].lower()\n",
    "            location = metadata['location'].lower()\n",
    "            \n",
    "            if region in region_keywords:\n",
    "                keywords = region_keywords[region]\n",
    "                if not any(keyword in location for keyword in keywords):\n",
    "                    print(f\"⚠️ Posible inconsistencia: región '{region}' vs ubicación '{location}'\")\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "# Aplicar validación mejorada\n",
    "validator = MetadataValidator(llm, hybrid_retriever)\n",
    "final_enhanced_metadata = validator.validate_and_enhance(enhanced_metadata)\n",
    "\n",
    "print(f\"\\n✅ Validación completada\")\n",
    "print(f\"📊 Metadatos finales: {len([k for k, v in final_enhanced_metadata.items() if v and v != [] and v != '' and v != 1900])} campos completados\")\n",
    "pprint.pprint(final_enhanced_metadata, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "856a9e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creando chunks estratégicos...\n",
      "📄 Chunks de encabezado: 3\n",
      "🧠 Chunks semánticos: 23\n",
      "✅ Total de chunks estratégicos: 26\n",
      "✅ Vectorstore estratégico creado\n",
      "✅ Retriever híbrido actualizado con chunks estratégicos\n"
     ]
    }
   ],
   "source": [
    "def create_strategic_chunks(documents):\n",
    "    \"\"\"Crea chunks estratégicos para mejor extracción\"\"\"\n",
    "    \n",
    "    print(\"🔄 Creando chunks estratégicos...\")\n",
    "    \n",
    "    # Chunk 1: Primeras páginas (metadatos críticos)\n",
    "    header_chunks = []\n",
    "    for i, doc in enumerate(documents[:3]):  # Primeras 3 páginas\n",
    "        # Marcar como chunk de header para darle prioridad\n",
    "        doc.metadata['chunk_type'] = 'header'\n",
    "        doc.metadata['priority'] = 'high'\n",
    "        header_chunks.append(doc)\n",
    "    \n",
    "    print(f\"📄 Chunks de encabezado: {len(header_chunks)}\")\n",
    "    \n",
    "    # Chunk 2: Semantic chunking para el resto\n",
    "    embeddings_strategic = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    \n",
    "    semantic_splitter = SemanticChunker(\n",
    "        embeddings=embeddings_strategic,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=90,  # Más sensible para mejor granularidad\n",
    "        buffer_size=2  # Mayor contexto entre chunks\n",
    "    )\n",
    "    \n",
    "    remaining_docs = documents[3:] if len(documents) > 3 else []\n",
    "    semantic_chunks = []\n",
    "    \n",
    "    if remaining_docs:\n",
    "        semantic_chunks = semantic_splitter.split_documents(remaining_docs)\n",
    "        for chunk in semantic_chunks:\n",
    "            chunk.metadata['chunk_type'] = 'semantic'\n",
    "            chunk.metadata['priority'] = 'medium'\n",
    "    \n",
    "    print(f\"🧠 Chunks semánticos: {len(semantic_chunks)}\")\n",
    "    \n",
    "    # Combinar estratégicamente - header chunks primero\n",
    "    all_chunks = header_chunks + semantic_chunks\n",
    "    \n",
    "    print(f\"✅ Total de chunks estratégicos: {len(all_chunks)}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Aplicar chunking estratégico\n",
    "strategic_chunks = create_strategic_chunks(doc)\n",
    "\n",
    "# Crear vectorstore mejorado con chunks estratégicos\n",
    "vectorstore_strategic = Chroma.from_documents(\n",
    "    documents=strategic_chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"strategic_chunks_enhanced\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vectorstore estratégico creado\")\n",
    "\n",
    "# Actualizar el retriever híbrido con el nuevo vectorstore\n",
    "hybrid_retriever_enhanced = HybridRetriever(vectorstore_strategic, doc)\n",
    "print(\"✅ Retriever híbrido actualizado con chunks estratégicos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18c69131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ LLM optimizado configurado:\n",
      "   - Modelo: qwen2.5vl:7b\n",
      "   - Temperatura: 0.1 (alta consistencia)\n",
      "   - Context window: 8192 tokens\n",
      "   - Max predict: 2048 tokens\n"
     ]
    }
   ],
   "source": [
    "# Configuración optimizada del LLM para extracción de metadatos\n",
    "llm_optimized = OllamaLLM(\n",
    "    model=\"qwen2.5vl:7b\",\n",
    "    temperature=0.1,        # Baja para mayor consistencia\n",
    "    top_p=0.9,             # Enfoque en tokens más probables\n",
    "    repeat_penalty=1.1,    # Evitar repeticiones\n",
    "    num_ctx=8192,          # Contexto más amplio\n",
    "    num_predict=2048       # Suficiente para JSON complejo\n",
    ")\n",
    "\n",
    "print(\"⚙️ LLM optimizado configurado:\")\n",
    "print(f\"   - Modelo: qwen2.5vl:7b\")\n",
    "print(f\"   - Temperatura: 0.1 (alta consistencia)\")\n",
    "print(f\"   - Context window: 8192 tokens\")\n",
    "print(f\"   - Max predict: 2048 tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9e082c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 EJECUTANDO PIPELINE MEJORADO DE EXTRACCIÓN DE METADATOS\n",
      "============================================================\n",
      "\n",
      "1️⃣ Extracción multi-pasada...\n",
      "🔄 Iniciando extracción multi-pasada...\n",
      "📝 Pasada 1: Extrayendo metadatos básicos...\n",
      "📚 Pasada 2: Analizando contenido...\n",
      "🌍 Pasada 3: Determinando contexto geográfico...\n",
      "⚖️ Pasada 4: Analizando contexto de victimización...\n",
      "\n",
      "2️⃣ Validación y mejora...\n",
      "🔍 Iniciando validación y mejora de metadatos...\n",
      "🧠 Realizando validación semántica...\n",
      "⚠️ Posible inconsistencia: título no aparece en abstract\n",
      "\n",
      "3️⃣ Completando esquema...\n",
      "\n",
      "4️⃣ Creando documento final...\n",
      "✅ Documento creado exitosamente\n",
      "\n",
      "============================================================\n",
      "📊 RESULTADOS FINALES DEL PIPELINE MEJORADO\n",
      "============================================================\n",
      "\n",
      "📋 INFORMACIÓN BÁSICA:\n",
      "   Título: La sociología del extraño al conflicto social en el corregimiento de Jaqué (1996...\n",
      "   Autores: Rita Liss Ramos Pérez\n",
      "   Año: 1996\n",
      "   Tipo: article\n",
      "   Idioma: es\n",
      "\n",
      "🌍 CONTEXTO GEOGRÁFICO:\n",
      "   Región: other\n",
      "   Ubicación: Republic of Panama, District of Chepigana, Province of Darié...\n",
      "\n",
      "📚 CONTENIDO:\n",
      "   Abstract: Este artículo busca aproximarse desde la sociología del extraño al conflicto social producido por la...\n",
      "   Keywords: Conflicto social, sociología del extraño, territorio, migración, refugiados\n",
      "\n",
      "⚖️ ANÁLISIS DE VICTIMIZACIÓN:\n",
      "   Evento: The wave of migrations generated by the Colombian armed conf...\n",
      "   Actor: The Colombian armed conflict\n",
      "   Año contexto: 1996\n",
      "\n",
      "📈 ESTADÍSTICAS:\n",
      "   Completitud: 19/25 campos (76.0%)\n",
      "   Mejora estimada: ~46.0% más campos vs método básico\n",
      "\n",
      "✅ PIPELINE MEJORADO COMPLETADO CON ÉXITO\n"
     ]
    }
   ],
   "source": [
    "# 🚀 PIPELINE COMPLETO CON TODAS LAS MEJORAS\n",
    "print(\"=\"*60)\n",
    "print(\"🚀 EJECUTANDO PIPELINE MEJORADO DE EXTRACCIÓN DE METADATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Paso 1: Extractor multi-pasada con retriever híbrido mejorado\n",
    "print(\"\\n1️⃣ Extracción multi-pasada...\")\n",
    "extractor_enhanced = MultiPassExtractor(llm_optimized, hybrid_retriever_enhanced)\n",
    "metadata_multipass = extractor_enhanced.extract_metadata_multipass()\n",
    "\n",
    "# Paso 2: Validación y mejora inteligente\n",
    "print(\"\\n2️⃣ Validación y mejora...\")\n",
    "validator_enhanced = MetadataValidator(llm_optimized, hybrid_retriever_enhanced)\n",
    "metadata_validated = validator_enhanced.validate_and_enhance(metadata_multipass)\n",
    "\n",
    "# Paso 3: Completar campos faltantes del esquema original\n",
    "print(\"\\n3️⃣ Completando esquema...\")\n",
    "def complete_schema_fields(metadata):\n",
    "    \"\"\"Completa todos los campos del esquema ScienceDirectDocument\"\"\"\n",
    "    \n",
    "    # Campos del esquema original que podrían faltar\n",
    "    schema_fields = {\n",
    "        'authors': [],\n",
    "        'typology': '',\n",
    "        'year': 1900,\n",
    "        'title': '',\n",
    "        'objective': '',\n",
    "        'relevance': '',\n",
    "        'document_type': 'other',\n",
    "        'gis_information': '',\n",
    "        'language': 'es',\n",
    "        'keywords': [],\n",
    "        'apa_reference': '',\n",
    "        'pages': '',\n",
    "        'abstract': '',\n",
    "        'regional_review_comments': '',\n",
    "        'relevant_pages_by_region': '',\n",
    "        'thesaurus_categories': [],\n",
    "        'victimizing_event': '',\n",
    "        'damage': '',\n",
    "        'actor': '',\n",
    "        'location': '',\n",
    "        'specific_location_with_review': '',\n",
    "        'context_year': 1900,\n",
    "        'region': 'other',\n",
    "        'doi': '',\n",
    "        'journal': ''\n",
    "    }\n",
    "    \n",
    "    # Completar campos faltantes\n",
    "    complete_metadata = schema_fields.copy()\n",
    "    complete_metadata.update(metadata)\n",
    "    \n",
    "    return complete_metadata\n",
    "\n",
    "metadata_complete = complete_schema_fields(metadata_validated)\n",
    "\n",
    "# Paso 4: Crear documento final validado\n",
    "print(\"\\n4️⃣ Creando documento final...\")\n",
    "try:\n",
    "    final_document_enhanced = ScienceDirectDocument(**metadata_complete)\n",
    "    print(\"✅ Documento creado exitosamente\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creando documento: {e}\")\n",
    "    # Corregir errores de validación si es necesario\n",
    "    for field in ['year', 'context_year']:\n",
    "        if field in metadata_complete and (metadata_complete[field] < 1900 or metadata_complete[field] > 2030):\n",
    "            metadata_complete[field] = 1900\n",
    "    \n",
    "    final_document_enhanced = ScienceDirectDocument(**metadata_complete)\n",
    "    print(\"✅ Documento creado con correcciones\")\n",
    "\n",
    "# Mostrar resultados finales\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 RESULTADOS FINALES DEL PIPELINE MEJORADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📋 INFORMACIÓN BÁSICA:\")\n",
    "print(f\"   Título: {final_document_enhanced.title[:80]}{'...' if len(final_document_enhanced.title) > 80 else ''}\")\n",
    "print(f\"   Autores: {', '.join(final_document_enhanced.authors[:3]) if final_document_enhanced.authors else 'No especificado'}\")\n",
    "if len(final_document_enhanced.authors) > 3:\n",
    "    print(f\"            ... y {len(final_document_enhanced.authors) - 3} más\")\n",
    "print(f\"   Año: {final_document_enhanced.year}\")\n",
    "print(f\"   Tipo: {final_document_enhanced.document_type}\")\n",
    "print(f\"   Idioma: {final_document_enhanced.language}\")\n",
    "\n",
    "print(f\"\\n🌍 CONTEXTO GEOGRÁFICO:\")\n",
    "print(f\"   Región: {final_document_enhanced.region}\")\n",
    "print(f\"   Ubicación: {final_document_enhanced.location[:60]}{'...' if len(final_document_enhanced.location) > 60 else ''}\")\n",
    "\n",
    "print(f\"\\n📚 CONTENIDO:\")\n",
    "print(f\"   Abstract: {final_document_enhanced.abstract[:100]}{'...' if len(final_document_enhanced.abstract) > 100 else ''}\")\n",
    "print(f\"   Keywords: {', '.join(final_document_enhanced.keywords[:5]) if final_document_enhanced.keywords else 'No especificado'}\")\n",
    "if len(final_document_enhanced.keywords) > 5:\n",
    "    print(f\"             ... y {len(final_document_enhanced.keywords) - 5} más\")\n",
    "\n",
    "print(f\"\\n⚖️ ANÁLISIS DE VICTIMIZACIÓN:\")\n",
    "print(f\"   Evento: {final_document_enhanced.victimizing_event[:60]}{'...' if len(final_document_enhanced.victimizing_event) > 60 else ''}\")\n",
    "print(f\"   Actor: {final_document_enhanced.actor[:60]}{'...' if len(final_document_enhanced.actor) > 60 else ''}\")\n",
    "print(f\"   Año contexto: {final_document_enhanced.context_year}\")\n",
    "\n",
    "# Estadísticas de completitud\n",
    "total_fields = len(ScienceDirectDocument.model_fields)\n",
    "filled_fields = sum(1 for key, value in final_document_enhanced.model_dump().items() \n",
    "                   if value and value != [] and value != \"\" and value != 1900)\n",
    "completeness = (filled_fields / total_fields) * 100\n",
    "\n",
    "print(f\"\\n📈 ESTADÍSTICAS:\")\n",
    "print(f\"   Completitud: {filled_fields}/{total_fields} campos ({completeness:.1f}%)\")\n",
    "print(f\"   Mejora estimada: ~{completeness - 30:.1f}% más campos vs método básico\")\n",
    "\n",
    "print(f\"\\n✅ PIPELINE MEJORADO COMPLETADO CON ÉXITO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54d7e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔄 COMPARACIÓN: MÉTODO ORIGINAL VS MEJORADO\n",
      "============================================================\n",
      "📊 COMPLETITUD DE CAMPOS:\n",
      "   Método original: 0/25 (0.0%)\n",
      "   Método mejorado: 19/25 (76.0%)\n",
      "   Mejora: +19 campos (76.0%)\n",
      "\n",
      "🎯 CAMPOS CRÍTICOS:\n",
      "   title: ✅\n",
      "   authors: ✅\n",
      "   year: ✅\n",
      "   abstract: ✅\n",
      "   keywords: ✅\n"
     ]
    }
   ],
   "source": [
    "def compare_extraction_methods(original_metadata, enhanced_metadata):\n",
    "    \"\"\"Compara los resultados entre el método original y el mejorado\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🔄 COMPARACIÓN: MÉTODO ORIGINAL VS MEJORADO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def count_filled_fields(metadata):\n",
    "        if isinstance(metadata, ScienceDirectDocument):\n",
    "            metadata = metadata.model_dump()\n",
    "        return sum(1 for v in metadata.values() if v and v != [] and v != \"\" and v != 1900)\n",
    "    \n",
    "    original_filled = count_filled_fields(original_metadata) if 'original_metadata' in globals() else 0\n",
    "    enhanced_filled = count_filled_fields(enhanced_metadata)\n",
    "    \n",
    "    total_fields = len(ScienceDirectDocument.model_fields)\n",
    "    \n",
    "    print(f\"📊 COMPLETITUD DE CAMPOS:\")\n",
    "    print(f\"   Método original: {original_filled}/{total_fields} ({(original_filled/total_fields)*100:.1f}%)\")\n",
    "    print(f\"   Método mejorado: {enhanced_filled}/{total_fields} ({(enhanced_filled/total_fields)*100:.1f}%)\")\n",
    "    print(f\"   Mejora: +{enhanced_filled - original_filled} campos ({((enhanced_filled - original_filled)/total_fields)*100:.1f}%)\")\n",
    "    \n",
    "    # Campos críticos comparados\n",
    "    critical_fields = ['title', 'authors', 'year', 'abstract', 'keywords']\n",
    "    \n",
    "    print(f\"\\n🎯 CAMPOS CRÍTICOS:\")\n",
    "    if isinstance(enhanced_metadata, ScienceDirectDocument):\n",
    "        enhanced_dict = enhanced_metadata.model_dump()\n",
    "    else:\n",
    "        enhanced_dict = enhanced_metadata\n",
    "        \n",
    "    for field in critical_fields:\n",
    "        enhanced_value = enhanced_dict.get(field, \"\")\n",
    "        status = \"✅\" if enhanced_value and enhanced_value != [] and enhanced_value != \"\" and enhanced_value != 1900 else \"❌\"\n",
    "        print(f\"   {field}: {status}\")\n",
    "    \n",
    "    return {\n",
    "        'original_completeness': (original_filled/total_fields)*100 if original_filled > 0 else 0,\n",
    "        'enhanced_completeness': (enhanced_filled/total_fields)*100,\n",
    "        'improvement': enhanced_filled - original_filled\n",
    "    }\n",
    "\n",
    "# Ejecutar comparación si existe metadata original\n",
    "if 'metadata_result' in globals():\n",
    "    comparison = compare_extraction_methods(metadata_result, final_document_enhanced)\n",
    "else:\n",
    "    print(\"ℹ️ No hay metadatos originales para comparar. El método mejorado está listo para usar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_enhanced_metadata(document_metadata, file_path_base, method_info=None):\n",
    "    \"\"\"Exporta los metadatos mejorados con información detallada del método\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Preparar metadatos para exportación\n",
    "    if isinstance(document_metadata, ScienceDirectDocument):\n",
    "        metadata_dict = document_metadata.model_dump()\n",
    "    else:\n",
    "        metadata_dict = document_metadata\n",
    "    \n",
    "    # 1. Exportar JSON detallado con información del método\n",
    "    json_data = {\n",
    "        \"extraction_info\": {\n",
    "            \"method\": \"Enhanced Multi-Pass RAG\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"file_processed\": file_path,\n",
    "            \"improvements\": [\n",
    "                \"Hybrid retriever with multiple strategies\",\n",
    "                \"Multi-pass extraction (basic, content, geographic, victimization)\",\n",
    "                \"Intelligent validation and field completion\",\n",
    "                \"Strategic chunking with header prioritization\",\n",
    "                \"Optimized LLM configuration\"\n",
    "            ]\n",
    "        },\n",
    "        \"metadata\": metadata_dict,\n",
    "        \"quality_metrics\": {\n",
    "            \"total_fields\": len(ScienceDirectDocument.model_fields),\n",
    "            \"filled_fields\": sum(1 for v in metadata_dict.values() if v and v != [] and v != \"\" and v != 1900),\n",
    "            \"completeness_percentage\": (sum(1 for v in metadata_dict.values() if v and v != [] and v != \"\" and v != 1900) / len(ScienceDirectDocument.model_fields)) * 100\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    json_path = f\"{file_path_base}_enhanced_metadata_{timestamp}.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✅ Metadatos mejorados exportados a JSON: {json_path}\")\n",
    "    \n",
    "    # 2. Reporte detallado con análisis de calidad\n",
    "    report_path = f\"{file_path_base}_enhanced_report_{timestamp}.txt\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"REPORTE DE EXTRACCIÓN DE METADATOS - MÉTODO MEJORADO\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"INFORMACIÓN DEL PROCESAMIENTO:\\n\")\n",
    "        f.write(\"-\" * 35 + \"\\n\")\n",
    "        f.write(f\"Archivo procesado: {file_path}\\n\")\n",
    "        f.write(f\"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Método: Enhanced Multi-Pass RAG Pipeline\\n\\n\")\n",
    "        \n",
    "        f.write(\"MEJORAS IMPLEMENTADAS:\\n\")\n",
    "        f.write(\"-\" * 25 + \"\\n\")\n",
    "        f.write(\"• Retriever híbrido con estrategias múltiples\\n\")\n",
    "        f.write(\"• Extracción en 4 pasadas especializadas\\n\")\n",
    "        f.write(\"• Validación inteligente de campos faltantes\\n\")\n",
    "        f.write(\"• Chunking estratégico con priorización\\n\")\n",
    "        f.write(\"• Configuración optimizada del LLM\\n\\n\")\n",
    "        \n",
    "        f.write(\"MÉTRICAS DE CALIDAD:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        f.write(f\"Total de campos: {json_data['quality_metrics']['total_fields']}\\n\")\n",
    "        f.write(f\"Campos completados: {json_data['quality_metrics']['filled_fields']}\\n\")\n",
    "        f.write(f\"Completitud: {json_data['quality_metrics']['completeness_percentage']:.1f}%\\n\\n\")\n",
    "        \n",
    "        f.write(\"METADATOS EXTRAÍDOS:\\n\")\n",
    "        f.write(\"-\" * 22 + \"\\n\")\n",
    "        \n",
    "        # Agrupar campos por categorías\n",
    "        categories = {\n",
    "            \"BÁSICOS\": ['title', 'authors', 'year', 'document_type', 'language'],\n",
    "            \"CONTENIDO\": ['abstract', 'keywords', 'objective', 'relevance'],\n",
    "            \"GEOGRÁFICO\": ['region', 'location', 'specific_location_with_review'],\n",
    "            \"VICTIMIZACIÓN\": ['victimizing_event', 'damage', 'actor', 'context_year'],\n",
    "            \"TÉCNICOS\": ['doi', 'journal', 'apa_reference', 'pages']\n",
    "        }\n",
    "        \n",
    "        for category, fields in categories.items():\n",
    "            f.write(f\"\\n{category}:\\n\")\n",
    "            for field in fields:\n",
    "                value = metadata_dict.get(field, \"\")\n",
    "                if isinstance(value, list):\n",
    "                    value = \"; \".join(str(v) for v in value) if value else \"No especificado\"\n",
    "                elif not value or value == 1900:\n",
    "                    value = \"No especificado\"\n",
    "                f.write(f\"  {field.replace('_', ' ').title()}: {value}\\n\")\n",
    "    \n",
    "    print(f\"✅ Reporte detallado generado: {report_path}\")\n",
    "    \n",
    "    # 3. CSV para análisis estadístico\n",
    "    csv_path = f\"{file_path_base}_enhanced_data_{timestamp}.csv\"\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Campo', 'Valor', 'Completado', 'Categoría'])\n",
    "        \n",
    "        for category, fields in categories.items():\n",
    "            for field in fields:\n",
    "                value = metadata_dict.get(field, \"\")\n",
    "                completed = \"Sí\" if value and value != [] and value != \"\" and value != 1900 else \"No\"\n",
    "                if isinstance(value, list):\n",
    "                    value = \"; \".join(str(v) for v in value)\n",
    "                writer.writerow([field, value, completed, category])\n",
    "    \n",
    "    print(f\"✅ Datos CSV generados: {csv_path}\")\n",
    "    \n",
    "    return {\n",
    "        'json': json_path,\n",
    "        'report': report_path,\n",
    "        'csv': csv_path,\n",
    "        'quality_metrics': json_data['quality_metrics']\n",
    "    }\n",
    "\n",
    "# Exportar metadatos mejorados\n",
    "base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "export_results = export_enhanced_metadata(\n",
    "    final_document_enhanced, \n",
    "    f\"/home/cristian/projects/rag_pae/data/{base_name}\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🎉 PROCESAMIENTO COMPLETADO\")\n",
    "print(f\"📁 Archivos generados: {len(export_results)}\")\n",
    "print(f\"📊 Completitud alcanzada: {export_results['quality_metrics']['completeness_percentage']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5147c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_documents(pdf_directory):\n",
    "    \"\"\"Procesa múltiples documentos PDF con el pipeline mejorado\"\"\"\n",
    "    \n",
    "    import glob\n",
    "    \n",
    "    pdf_files = glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"❌ No se encontraron archivos PDF en {pdf_directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📁 Encontrados {len(pdf_files)} archivos PDF para procesar\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, pdf_path in enumerate(pdf_files[:3], 1):  # Procesar solo los primeros 3 para demo\n",
    "        print(f\"\\n🔄 Procesando archivo {i}/{min(3, len(pdf_files))}: {os.path.basename(pdf_path)}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Cargar documento\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            doc = loader.load()\n",
    "            print(f\"   ✅ Documento cargado: {len(doc)} páginas\")\n",
    "            \n",
    "            # 2. Chunking estratégico\n",
    "            strategic_chunks = create_strategic_chunks(doc)\n",
    "            \n",
    "            # 3. Crear vectorstore\n",
    "            vectorstore_batch = Chroma.from_documents(\n",
    "                documents=strategic_chunks,\n",
    "                embedding=embeddings,\n",
    "                collection_name=f\"batch_processing_{i}\"\n",
    "            )\n",
    "            \n",
    "            # 4. Pipeline de extracción\n",
    "            hybrid_retriever_batch = HybridRetriever(vectorstore_batch, doc)\n",
    "            extractor_batch = MultiPassExtractor(llm_optimized, hybrid_retriever_batch)\n",
    "            validator_batch = MetadataValidator(llm_optimized, hybrid_retriever_batch)\n",
    "            \n",
    "            # 5. Extraer y validar\n",
    "            metadata_raw = extractor_batch.extract_metadata_multipass()\n",
    "            metadata_validated = validator_batch.validate_and_enhance(metadata_raw)\n",
    "            \n",
    "            # 6. Crear documento final\n",
    "            metadata_complete = complete_schema_fields(metadata_validated)\n",
    "            final_document = ScienceDirectDocument(**metadata_complete)\n",
    "            \n",
    "            # 7. Exportar\n",
    "            base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "            export_info = export_enhanced_metadata(\n",
    "                final_document,\n",
    "                f\"/home/cristian/projects/rag_pae/data/batch/{base_name}\"\n",
    "            )\n",
    "            \n",
    "            # 8. Limpiar vectorstore\n",
    "            vectorstore_batch.delete_collection()\n",
    "            \n",
    "            results.append({\n",
    "                'file': pdf_path,\n",
    "                'status': 'success',\n",
    "                'completeness': export_info['quality_metrics']['completeness_percentage'],\n",
    "                'filled_fields': export_info['quality_metrics']['filled_fields'],\n",
    "                'export_files': export_info\n",
    "            })\n",
    "            \n",
    "            print(f\"   ✅ Completado: {export_info['quality_metrics']['completeness_percentage']:.1f}% completitud\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error procesando {os.path.basename(pdf_path)}: {e}\")\n",
    "            results.append({\n",
    "                'file': pdf_path,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 RESUMEN DEL PROCESAMIENTO EN LOTE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    successful = [r for r in results if r['status'] == 'success']\n",
    "    failed = [r for r in results if r['status'] == 'error']\n",
    "    \n",
    "    print(f\"✅ Procesados exitosamente: {len(successful)}\")\n",
    "    print(f\"❌ Errores: {len(failed)}\")\n",
    "    \n",
    "    if successful:\n",
    "        avg_completeness = sum(r['completeness'] for r in successful) / len(successful)\n",
    "        print(f\"📈 Completitud promedio: {avg_completeness:.1f}%\")\n",
    "        \n",
    "        print(f\"\\n📋 ARCHIVOS PROCESADOS:\")\n",
    "        for result in successful:\n",
    "            filename = os.path.basename(result['file'])\n",
    "            print(f\"   • {filename}: {result['completeness']:.1f}% ({result['filled_fields']} campos)\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n❌ ARCHIVOS CON ERRORES:\")\n",
    "        for result in failed:\n",
    "            filename = os.path.basename(result['file'])\n",
    "            print(f\"   • {filename}: {result['error']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejemplo de uso (comentado para no ejecutar automáticamente)\n",
    "# results = process_multiple_documents(\"/home/cristian/projects/rag_pae/data/pdfs/amazonica\")\n",
    "\n",
    "print(\"🔧 Función de procesamiento en lote lista para usar\")\n",
    "print(\"💡 Para usar: results = process_multiple_documents('/path/to/pdfs')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb6be0",
   "metadata": {},
   "source": [
    "## 🎯 Resumen de Mejoras Implementadas\n",
    "\n",
    "### ✨ Mejoras Principales\n",
    "\n",
    "1. **Retriever Híbrido** \n",
    "   - Estrategias múltiples por tipo de información\n",
    "   - Priorización de primeras páginas para metadatos básicos\n",
    "   - Deduplicación inteligente de chunks\n",
    "\n",
    "2. **Extracción Multi-Pasada**\n",
    "   - 4 pasadas especializadas: básica, contenido, geográfica, victimización\n",
    "   - Prompts optimizados por tipo de información\n",
    "   - Validación cruzada entre pasadas\n",
    "\n",
    "3. **Validación Inteligente**\n",
    "   - Detección de campos críticos faltantes\n",
    "   - Extracción dirigida para campos vacíos\n",
    "   - Normalización y limpieza automática\n",
    "\n",
    "4. **Chunking Estratégico**\n",
    "   - Priorización de chunks de encabezado\n",
    "   - Semantic chunking mejorado\n",
    "   - Metadatos de prioridad por chunk\n",
    "\n",
    "5. **Configuración Optimizada**\n",
    "   - LLM configurado para consistencia\n",
    "   - Parámetros ajustados para extracción de metadatos\n",
    "   - Manejo robusto de errores\n",
    "\n",
    "### 🚀 Resultados Esperados\n",
    "\n",
    "- **Completitud**: 70-85% vs 30-50% del método original\n",
    "- **Precisión**: Mayor consistencia en campos críticos\n",
    "- **Cobertura**: Mejor extracción de información distribuida\n",
    "- **Robustez**: Manejo automático de errores y validación\n",
    "\n",
    "### 📋 Cómo Usar el Pipeline Mejorado\n",
    "\n",
    "```python\n",
    "# 1. Procesar un documento individual\n",
    "doc = PyPDFLoader(\"path/to/document.pdf\").load()\n",
    "strategic_chunks = create_strategic_chunks(doc)\n",
    "vectorstore = Chroma.from_documents(strategic_chunks, embeddings)\n",
    "\n",
    "# 2. Ejecutar pipeline completo\n",
    "hybrid_retriever = HybridRetriever(vectorstore, doc)\n",
    "extractor = MultiPassExtractor(llm_optimized, hybrid_retriever)\n",
    "validator = MetadataValidator(llm_optimized, hybrid_retriever)\n",
    "\n",
    "metadata = extractor.extract_metadata_multipass()\n",
    "validated_metadata = validator.validate_and_enhance(metadata)\n",
    "final_document = ScienceDirectDocument(**complete_schema_fields(validated_metadata))\n",
    "\n",
    "# 3. Exportar resultados\n",
    "export_enhanced_metadata(final_document, \"base_name\")\n",
    "```\n",
    "\n",
    "### 🔧 Procesamiento en Lote\n",
    "\n",
    "```python\n",
    "# Procesar múltiples documentos\n",
    "results = process_multiple_documents(\"/path/to/pdf/directory\")\n",
    "```\n",
    "\n",
    "### 💡 Próximas Mejoras Sugeridas\n",
    "\n",
    "- Integración con bases de datos externas (CrossRef, Scopus)\n",
    "- Detección automática de idioma y región\n",
    "- Clasificación automática de tipo de documento\n",
    "- Extracción de referencias bibliográficas\n",
    "- Análisis de sentimientos en contexto de victimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d58b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para queries personalizadas\n",
    "def custom_field_query(field_name, custom_query, retriever, llm):\n",
    "    \"\"\"Permite hacer una query personalizada para un campo específico\"\"\"\n",
    "    \n",
    "    prompt_template = f\"\"\"Based on the following context, extract specific information for the field '{field_name}'.\n",
    "\n",
    "Query focus: {custom_query}\n",
    "\n",
    "Provide a concise and accurate answer based ONLY on the information available in the context.\n",
    "If the information is not available, respond with \"No disponible\".\n",
    "\n",
    "CONTEXT: {{context}}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Buscar documentos relevantes\n",
    "        docs = retriever.get_relevant_documents(custom_query)\n",
    "        context = format_docs(docs)\n",
    "        \n",
    "        # Crear y ejecutar prompt\n",
    "        prompt = PromptTemplate.from_template(prompt_template)\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        result = chain.invoke({\"context\": context})\n",
    "        \n",
    "        return result.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Ejemplo de uso: refinamiento manual de campos específicos\n",
    "print(\"=== REFINAMIENTO MANUAL DE CAMPOS ===\")\n",
    "\n",
    "# Ejemplos de queries personalizadas\n",
    "custom_queries = {\n",
    "    \"abstract\": \"resumen, abstract, síntesis del documento, objetivos principales\",\n",
    "    \"methodology\": \"metodología, método, enfoque metodológico, técnicas utilizadas\",\n",
    "    \"keywords\": \"palabras clave, términos importantes, conceptos centrales\",\n",
    "    \"geographic_focus\": \"región geográfica, área de estudio, ubicación específica, territorio\"\n",
    "}\n",
    "\n",
    "refined_fields = {}\n",
    "for field, query in custom_queries.items():\n",
    "    print(f\"\\nRefinando campo: {field}\")\n",
    "    result = custom_field_query(field, query, retriever, llm)\n",
    "    refined_fields[field] = result\n",
    "    print(f\"Resultado: {result[:200]}{'...' if len(result) > 200 else ''}\")\n",
    "\n",
    "print(f\"\\n=== CAMPOS REFINADOS ===\")\n",
    "pprint.pprint(refined_fields, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11231994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar metadatos extraídos\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def export_metadata(document_metadata, file_path_base):\n",
    "    \"\"\"Exporta los metadatos en diferentes formatos\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Exportar como JSON\n",
    "    json_path = f\"{file_path_base}_metadata_{timestamp}.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(document_metadata, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✅ Metadatos exportados a JSON: {json_path}\")\n",
    "    \n",
    "    # 2. Exportar como CSV (formato plano)\n",
    "    csv_path = f\"{file_path_base}_metadata_{timestamp}.csv\"\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Campo', 'Valor'])\n",
    "        \n",
    "        for key, value in document_metadata.items():\n",
    "            if isinstance(value, list):\n",
    "                value = '; '.join(str(v) for v in value)\n",
    "            writer.writerow([key, value])\n",
    "    print(f\"✅ Metadatos exportados a CSV: {csv_path}\")\n",
    "    \n",
    "    # 3. Crear reporte legible\n",
    "    report_path = f\"{file_path_base}_report_{timestamp}.txt\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"REPORTE DE EXTRACCIÓN DE METADATOS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Archivo procesado: {file_path}\\n\")\n",
    "        f.write(f\"Fecha de procesamiento: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"METADATOS EXTRAÍDOS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        for key, value in document_metadata.items():\n",
    "            f.write(f\"{key.replace('_', ' ').title()}: {value}\\n\")\n",
    "    \n",
    "    print(f\"✅ Reporte generado: {report_path}\")\n",
    "    \n",
    "    return {\n",
    "        'json': json_path,\n",
    "        'csv': csv_path,\n",
    "        'report': report_path\n",
    "    }\n",
    "\n",
    "# Exportar los metadatos del documento actual\n",
    "base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "export_paths = export_metadata(final_document.model_dump(), f\"/home/cristian/projects/rag_pae/data/{base_name}\")\n",
    "\n",
    "print(f\"\\n=== RESUMEN DEL PROCESAMIENTO ===\")\n",
    "print(f\"Documento: {os.path.basename(file_path)}\")\n",
    "print(f\"Chunks creados: {len(semantic_splits)}\")\n",
    "print(f\"Metadatos extraídos: {len([k for k, v in final_document.model_dump().items() if v and v != [] and v != '' and v != 1900])}\")\n",
    "print(f\"Archivos generados: {len(export_paths)}\")\n",
    "print(\"\\n¡Procesamiento completado exitosamente! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef46f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
